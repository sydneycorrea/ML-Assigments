{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Problem Set - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corre\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords  #pip install nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from gensim.models import word2vec  #pip install word2vec\n",
    "\n",
    "from wordcloud import WordCloud  #pip install wordcloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "\n",
    "from textblob import TextBlob  #Sentiment Analysis - pip install textblob\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "# Text pipeline and NLP packages\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords  #pip install nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from wordnet import wordnet\n",
    "\n",
    "import string\n",
    "\n",
    "from gensim import corpora, models \n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.test.utils import common_corpus\n",
    "\n",
    "from pyemd import emd   # pip install pyemd\n",
    "\n",
    "# SciKit NMF and LDA package and some easy text data\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Python package for Apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder  #pip install mlxtend\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPIS_docs= pd.read_csv('C:\\\\Users\\\\corre\\\\Desktop\\\\CSCI E-82\\\\PS2\\\\nips-papers\\\\papers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7241, 7)\n"
     ]
    }
   ],
   "source": [
    "print(NPIS_docs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id  year                                              title event_type  \\\n",
      "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
      "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
      "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
      "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
      "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
      "\n",
      "                                            pdf_name          abstract  \\\n",
      "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
      "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
      "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
      "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
      "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
      "\n",
      "                                          paper_text  \n",
      "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
      "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
      "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
      "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
      "4  Neural Network Ensembles, Cross\\nValidation, a...  \n"
     ]
    }
   ],
   "source": [
    "print(NPIS_docs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "with open('C:\\\\Users\\\\corre\\\\Desktop\\\\CSCI E-82\\\\PS2\\\\nips-papers\\\\papers.csv', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        [documents.append(l) for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5416330\n",
      "id,year,title,event_type,pdf_name,abstract,paper_text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5416330\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "doc_lower = [doc.lower() for doc in documents]\n",
    "doc_tokens = [tokenizer.tokenize(doc) for doc in doc_lower]\n",
    "\n",
    "print(len(doc_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '1987', 'self', 'organization', 'of', 'associative', 'database', 'and', 'its', 'applications', '1', 'self', 'organization', 'of', 'associative', 'database', 'and', 'its', 'applications', 'pdf', 'abstract', 'missing', '767']\n"
     ]
    }
   ],
   "source": [
    "print(doc_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "stops = set(stopwords.words(\"english\")) #stops\n",
    "\n",
    "#print(stops,\"\\n\") #Already defined in NLTK\n",
    "\n",
    "stops = stops.union(['i','br'])  # add some stopwords\n",
    "doc = []\n",
    "stopped_doc_tokens = []\n",
    "for doc in doc_tokens[1]:\n",
    "    if(doc not in stops): \n",
    "        print(stopped_doc_tokens.append(doc))\n",
    "    #print(doc)\n",
    "print(stopped_doc_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['id', 'year', 'title', 'event_type', 'pdf_name', 'abstract', 'paper_text'], ['1', '1987', 'self', 'organization', 'of', 'associative', 'database', 'and', 'its', 'applications', '1', 'self', 'organization', 'of', 'associative', 'database', 'and', 'its', 'applications', 'pdf', 'abstract', 'missing', '767'], [], ['self', 'organization', 'of', 'associative', 'database'], ['and', 'its', 'applications']]\n"
     ]
    }
   ],
   "source": [
    "print(doc_tokens[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5416330\n",
      "['id', 'year', 'title', 'event_type', 'pdf_name', 'abstract', 'paper_text']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemma_doc_tokens = []\n",
    "for doc in stopped_doc_tokens:\n",
    "    lemma_doc_tokens.append([lemmatizer.lemmatize(word) for word in doc])\n",
    "\n",
    "print(len(lemma_doc_tokens))\n",
    "print(lemma_doc_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5416330\n",
      "['id', 'year', 'titl', 'event_typ', 'pdf_name', 'abstract', 'paper_text']\n"
     ]
    }
   ],
   "source": [
    "stopstem_doc_tokens = []\n",
    "for doc in stopped_doc_tokens:\n",
    "    stopstem_doc_tokens.append([PorterStemmer().stem(word) for word in doc])\n",
    "\n",
    "print(len(stopstem_doc_tokens))\n",
    "print(stopstem_doc_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopped_doc_tokens.tail())\n",
    "print(lemma_doc_tokens.tail())\n",
    "print(stopstem_doc_tokens.tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
